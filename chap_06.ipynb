{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd0ba3b919a19380c03e01f18adb9941583c39169f4907f8bc2002c90bf52d7e13c",
   "display_name": "Python 3.8.10 64-bit ('optAlgos': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 6: Second-Order Methods"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "from dataclasses import dataclass\n",
    "import jax\n",
    "import numpy as np\n",
    "from typing import Type"
   ]
  },
  {
   "source": [
    "## Algorithm 6.1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newtons_method(gradient, hessian, x, epsilon, k_max):\n",
    "        delta = np.infty*np.ones(x.shape[0])\n",
    "        k = 1\n",
    "        while np.linalg.norm(delta) > epsilon and k <= k_max:\n",
    "                delta = np.linalg.solve(hessian(x),-gradient(x))\n",
    "                x -= delta\n",
    "                k += 1\n",
    "        return x"
   ]
  },
  {
   "source": [
    "### Example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[  0.29836422  28.81422853 -21.8234632 ]\n"
     ]
    }
   ],
   "source": [
    "def fun(x):\n",
    "    return jax.numpy.sin(x[0]*x[1])+jax.numpy.exp(x[1]+x[2])-x[2]\n",
    "\n",
    "x_0 = np.array([1.0, 2.0, 3.0])\n",
    "gradient = jax.grad(fun)\n",
    "hessian = jax.hessian(fun)\n",
    "epsilon = 0.0001\n",
    "k_max = 2\n",
    "x_1 = newtons_method(gradient, hessian, x_0, epsilon, k_max)\n",
    "print(x_1)"
   ]
  },
  {
   "source": [
    "## Algorithm 6.2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secant_method(derivative, x_0, x_1, epsilon):\n",
    "    g_0 = derivative(x_0)\n",
    "    delta = np.infty\n",
    "    while np.abs(delta) > epsilon:\n",
    "        g_1 = derivative(x_1)\n",
    "        delta = ((x_1-x_0)/(g_1-g_0))*g_1\n",
    "        x_0, x_1, g_0 = x_1, x_1 - delta, g_1\n",
    "\n",
    "    return x_1"
   ]
  },
  {
   "source": [
    "## Algorithm 6.3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DFP:\n",
    "    Q = np.identity(1)\n",
    "\n",
    "def init(M:Type[DFP], x):\n",
    "    m = x.shape[0]\n",
    "    M.Q = np.identity(m)\n",
    "    return M\n",
    "\n",
    "def step(M:Type[DFP], f, gradient, x):\n",
    "    Q, g = M.Q, gradient(x)\n",
    "    x_prime = line_search(f, x, np.dot(-Q, g) )\n",
    "    g_prime = gradient(x_prime)\n",
    "    # -- Vectors as columns -- #\n",
    "    x = x.reshape(-1,1)\n",
    "    x_prime = x_prime.reshape(-1,1)\n",
    "    g = g.reshape(-1,1)\n",
    "    g_prime = g_prime.reshape(-1,1)\n",
    "    # ------------------------ #\n",
    "    delta = x_prime - x\n",
    "    gamma = g_prime - g\n",
    "    Q = Q - np.dot(Q,np.dot(gamma,np.dot(gamma.T,Q)))/np.dot(gamma.T,np.dot(Q, gamma)) + np.dot(delta, delta.T)/np.dot(delta.T,gamma)\n",
    "    M.Q = Q\n",
    "    return x_prime.reshape(-1,)"
   ]
  },
  {
   "source": [
    "### Example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bracket_minimum(f, x=0, s=1e-2, k=2.0):\n",
    "    a, ya = x, f(x)\n",
    "    b, yb = a + s, f(a+s)\n",
    "    \n",
    "    if yb > ya:\n",
    "        a, b = b, a\n",
    "        ya, yb = yb, ya\n",
    "        s = -s\n",
    "    \n",
    "    while True:\n",
    "        c, yc = b + s, f(b+s)\n",
    "        if yc > yb:\n",
    "            return (a,c) if a < c else (c, a)\n",
    "        a, ya, b, yb = b, yb, c, yc\n",
    "\n",
    "def minimize(f_deriv, a, b, epsilon):\n",
    "    '''\n",
    "    Bisection algorithm for univariate optmization\n",
    "    '''\n",
    "    if a > b:\n",
    "        a, b = b, a\n",
    "    \n",
    "    ya, yb = f_deriv(a), f_deriv(b)\n",
    "\n",
    "    if ya == 0:\n",
    "        b = a\n",
    "    if yb == 0:\n",
    "        a = b\n",
    "\n",
    "    while b - a > epsilon:\n",
    "        x = (a+b)/2\n",
    "        y = f_deriv(x)\n",
    "        if y == 0:\n",
    "            a, b = x, x\n",
    "        elif y*ya> 0:\n",
    "            a = x\n",
    "        else:\n",
    "            b = x\n",
    "    \n",
    "    return (a+b)/2\n",
    "\n",
    "def line_search(f, x, d):\n",
    "    '''\n",
    "    One step of Exact Line Search\n",
    "    '''\n",
    "    objective = lambda alpha: f(x+alpha*d)\n",
    "\n",
    "    a, b = bracket_minimum(objective)\n",
    "\n",
    "    alpha = minimize(objective, a, b, 0.0001)\n",
    "\n",
    "    return x + alpha*d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[-0.43911442]\n [ 1.8882942 ]\n [ 0.00341964]]\n[ 1.01407747 -0.50323066  0.50664456]\n"
     ]
    }
   ],
   "source": [
    "def func(x):\n",
    "    return jax.numpy.sin(x[0]*x[1])+jax.numpy.exp(x[1]+x[2])-x[2]\n",
    "\n",
    "x_0 = np.array([1.0, 2.0, 3.0])\n",
    "gradient = jax.grad(func)\n",
    "M = DFP()\n",
    "M = init(M, x_0)\n",
    "x_1 = step(M, func, gradient, x_0)\n",
    "print(x_1)"
   ]
  },
  {
   "source": [
    "## Algorithm 6.4"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BFGS:\n",
    "    Q = np.identity(1)\n",
    "\n",
    "def init(M:Type[BFGS], x):\n",
    "    m = x.shape[0]\n",
    "    M.Q = np.identity(m)\n",
    "    return M\n",
    "\n",
    "def step(M:Type[BFGS], f, gradient, x):\n",
    "    Q, g = M.Q, gradient(x)\n",
    "    x_prime = line_search(f, x, np.dot(-Q, g) )\n",
    "    g_prime = gradient(x_prime)\n",
    "    # -- Vectors as columns -- #\n",
    "    x = x.reshape(-1,1)\n",
    "    x_prime = x_prime.reshape(-1,1)\n",
    "    g = g.reshape(-1,1)\n",
    "    g_prime = g_prime.reshape(-1,1)\n",
    "    # ------------------------ #\n",
    "    delta = x_prime - x\n",
    "    gamma = g_prime - g\n",
    "    prt1 = (np.dot(delta, np.dot(gamma.T,Q)) + np.dot(Q, np.dot(gamma,delta.T)))/np.dot(delta.T, gamma)\n",
    "    prt2 = ( 1 + np.dot(gamma.T,np.dot(Q,gamma))/np.dot(delta.T, gamma) )\n",
    "    prt3 = np.dot(delta, delta.T)/np.dot(delta.T, gamma)\n",
    "    Q = Q - prt1 + prt2*prt3\n",
    "    M.Q = Q\n",
    "    return x_prime.reshape(-1,)"
   ]
  },
  {
   "source": [
    "## Algorithm 6.5"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LimitedMemoryBFGS:\n",
    "    m: int\n",
    "    deltas = [] #ẟs\n",
    "    gammas = [] #γs\n",
    "    qs = []\n",
    "\n",
    "def step(M:Type[LimitedMemoryBFGS], f, gradient, x):\n",
    "    deltas, gammas, qs = M.deltas, M.gammas, M.qs\n",
    "    g = gradient(x)\n",
    "    m = len(M.deltas)\n",
    "    print(m)\n",
    "    if m >0:\n",
    "        q = g.reshape(-1,1)\n",
    "        for i in range(m-1,-1,-1):\n",
    "            qs[i] = copy(q)\n",
    "            q -=  (np.dot(deltas[i].T, q) / np.dot(gammas[i].T, deltas[i]))[0,0] * gammas[i] \n",
    "        z = (gammas[m-1]*deltas[m-1]*q) / np.dot(gammas[m-1].T,gammas[m-1])\n",
    "        for i in range(m):\n",
    "            z += np.dot(deltas[i], ( np.dot(deltas[i].T,qs[i]) - np.dot(gammas[i].T,z) ) / np.dot(gammas[i].T, deltas[i]) )\n",
    "        x_prime = line_search(f, x, -z.reshape(-1,))\n",
    "    else:\n",
    "        x_prime = line_search(f, x, -g)\n",
    "    g_prime = gradient(x_prime)\n",
    "    deltas.insert(-1, x_prime.reshape(-1,1) - x.reshape(-1,1) )\n",
    "    gammas.insert(-1, g_prime.reshape(-1,1) - g.reshape(-1,1))\n",
    "    qs.insert(-1,np.zeros((x.shape[0],1)))\n",
    "    while len(deltas) > M.m:\n",
    "        deltas.pop(0)\n",
    "        gammas.pop(0)\n",
    "        qs.pop(0)\n",
    "    M.gammas = gammas\n",
    "    M.deltas = deltas\n",
    "    M.qs = qs\n",
    "    return x_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1\n",
      "[[-9.1543436e-05]\n",
      " [ 2.4279181e-02]\n",
      " [ 8.1006093e-03]]\n",
      "[ 1.0180019  -1.5440781   0.15937167]\n"
     ]
    }
   ],
   "source": [
    "def func(x):\n",
    "    return jax.numpy.sin(x[0]*x[1])+jax.numpy.exp(x[1]+x[2])-x[2]\n",
    "\n",
    "x_0 = np.array([1.0, 2.0, 3.0])\n",
    "gradient = jax.grad(func)\n",
    "M = LimitedMemoryBFGS(m=10)\n",
    "x_1 = step(M, func, gradient, x_0)\n",
    "x_2 = step(M, func, gradient, x_1)\n",
    "print(x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('optAlgos': conda)"
  },
  "interpreter": {
   "hash": "ba3b919a19380c03e01f18adb9941583c39169f4907f8bc2002c90bf52d7e13c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 5: First-Order Methods"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import jax"
   ]
  },
  {
   "source": [
    "## Algorithm 5.1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "@dataclass\n",
    "class GradientDescent:\n",
    "    alpha : float # α\n",
    "\n",
    "def step(M:Type[GradientDescent], f, gradient, x):\n",
    "    alpha, g = M.alpha, gradient(x)\n",
    "    return x - alpha*g"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "source": [
    "### Example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "[  1.0832294 -12.799701  -11.741317 ]\n"
     ]
    }
   ],
   "source": [
    "def fun(x):\n",
    "    return jax.numpy.sin(x[0]*x[1])+jax.numpy.exp(x[1]+x[2])-x[2]\n",
    "\n",
    "x_0 = np.array([1.0, 2.0, 3.0])\n",
    "gradient = jax.grad(fun)\n",
    "M = GradientDescent(0.1)\n",
    "x_1 = step(M, fun, gradient, x_0) \n",
    "print(x_1)"
   ]
  },
  {
   "source": [
    "## Algorithm 5.2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConjugateGradientDescent:\n",
    "\n",
    "    def __init__(self, g):\n",
    "        self.g = g \n",
    "        self.d = -self.g\n",
    "    \n",
    "    def step(self, f, gradient, x):\n",
    "        g_prime = gradient(x)\n",
    "        beta = max(0, np.dot(g_prime, g_prime - self.g)/(np.dot(self.g, self.g)))\n",
    "        d_prime = -g_prime + beta*self.d\n",
    "        x_prime = line_search(f, x, d_prime)\n",
    "        self.d = d_prime\n",
    "        self.g = g_prime\n",
    "        \n",
    "        return x_prime\n",
    "    \n",
    "    def first_step(self, f, x):\n",
    "        x_prime = line_search(f, x, self.d)\n",
    "        return x_prime"
   ]
  },
  {
   "source": [
    "### Example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 1.2423997  -1.4850699   0.50486636]\n"
     ]
    }
   ],
   "source": [
    "def bracket_minimum(f, x=0, s=1e-2, k=2.0):\n",
    "    a, ya = x, f(x)\n",
    "    b, yb = a + s, f(a+s)\n",
    "    \n",
    "    if yb > ya:\n",
    "        a, b = b, a\n",
    "        ya, yb = yb, ya\n",
    "        s = -s\n",
    "    \n",
    "    while True:\n",
    "        c, yc = b + s, f(b+s)\n",
    "        if yc > yb:\n",
    "            return (a,c) if a < c else (c, a)\n",
    "        a, ya, b, yb = b, yb, c, yc\n",
    "\n",
    "def minimize(f_deriv, a, b, epsilon):\n",
    "    '''\n",
    "    Bisection algorithm for univariate optmization\n",
    "    '''\n",
    "    if a > b:\n",
    "        a, b = b, a\n",
    "    \n",
    "    ya, yb = f_deriv(a), f_deriv(b)\n",
    "\n",
    "    if ya == 0:\n",
    "        b = a\n",
    "    if yb == 0:\n",
    "        a = b\n",
    "\n",
    "    while b - a > epsilon:\n",
    "        x = (a+b)/2\n",
    "        y = f_deriv(x)\n",
    "        if y == 0:\n",
    "            a, b = x, x\n",
    "        elif y*ya> 0:\n",
    "            a = x\n",
    "        else:\n",
    "            b = x\n",
    "    \n",
    "    return (a+b)/2\n",
    "\n",
    "def line_search(f, x, d):\n",
    "    '''\n",
    "    One step of Exact Line Search\n",
    "    '''\n",
    "    objective = lambda alpha: f(x+alpha*d)\n",
    "\n",
    "    a, b = bracket_minimum(objective)\n",
    "\n",
    "    alpha = minimize(objective, a, b, 0.0001)\n",
    "\n",
    "    return x + alpha*d\n",
    "\n",
    "def func(x):\n",
    "    return jax.numpy.sin(x[0]*x[1])+jax.numpy.exp(x[1]+x[2])-x[2]\n",
    "\n",
    "\n",
    "x_0 = np.array([1.0, 2.0, 3.0])\n",
    "\n",
    "gradient =  jax.grad(func) # ∇f\n",
    "g = gradient(x_0) # ∇f(x)\n",
    "\n",
    "model = ConjugateGradientDescent(g)\n",
    "\n",
    "x_1 = model.first_step(func, x_0)\n",
    "x_2 = model.step(func, gradient, x_1)\n",
    "print(x_2)"
   ]
  },
  {
   "source": [
    "## Algorithm 5.3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "\n",
    "    def __init__(self, alpha, beta, dimension):\n",
    "        self.alpha = alpha # α\n",
    "        self.beta = beta # β\n",
    "        self.v = np.zeros(dimension)\n",
    "\n",
    "    def step(self, gradient, x):\n",
    "        g = gradient(x)\n",
    "        self.v = self.beta*self.v - self.alpha*g\n",
    "        return x + self.v"
   ]
  },
  {
   "source": [
    "### Example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[  1.0832294 -12.799701  -11.741317 ]\n"
     ]
    }
   ],
   "source": [
    "def func(x):\n",
    "    return jax.numpy.sin(x[0]*x[1])+jax.numpy.exp(x[1]+x[2])-x[2]\n",
    "\n",
    "dim = 3\n",
    "alpha = 0.1\n",
    "beta = 0.3\n",
    "model = Momentum(alpha, beta, dim)\n",
    "\n",
    "x_0 = np.array([1.0, 2.0, 3.0])\n",
    "gradient = jax.grad(func)\n",
    "x_1 = model.step(gradient, x_0)\n",
    "print(x_1)"
   ]
  },
  {
   "source": [
    "## Algorithm 5.4"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NesterovMomentum:\n",
    "\n",
    "    def __init__(self, alpha, beta, dimension):\n",
    "        self.alpha = alpha # α\n",
    "        self.beta = beta # β\n",
    "        self.v = np.zeros(dimension)\n",
    "\n",
    "    def step(self, gradient, x):\n",
    "        g = gradient(x+self.beta*self.v)\n",
    "        self.v = self.beta*self.v - self.alpha*g\n",
    "        return x + self.v"
   ]
  },
  {
   "source": [
    "## Algorithm 5.5"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad:\n",
    "\n",
    "    def __init__(self, alpha, epsilon, dimension):\n",
    "        self.alpha = alpha # α\n",
    "        self.epsilon = epsilon # ϵ\n",
    "        self.s = np.zeros(dimension)\n",
    "\n",
    "    def step(self, gradient, x):\n",
    "        g = gradient(x)\n",
    "        self.s += g*g\n",
    "        return x - self.alpha / (np.sqrt(self.s) + self.epsilon)"
   ]
  },
  {
   "source": [
    "### Example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.8798501  1.99932431 2.99932163]\n"
     ]
    }
   ],
   "source": [
    "def func(x):\n",
    "    return jax.numpy.sin(x[0]*x[1])+jax.numpy.exp(x[1]+x[2])-x[2]\n",
    "\n",
    "alpha = 0.1\n",
    "epsilon = 1e-8\n",
    "dim = 3\n",
    "model = Adagrad(alpha, epsilon, dim)\n",
    "\n",
    "x_0 = np.array([1.0, 2.0, 3.0])\n",
    "gradient = jax.grad(func)\n",
    "x_1 = model.step(gradient, x_0)\n",
    "print(x_1)"
   ]
  },
  {
   "source": [
    "## Algorithm 5.6"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp:\n",
    "\n",
    "    def __init__(self, alpha, gamma, epsilon, dimension):\n",
    "        self.alpha = alpha # α\n",
    "        self.gamma = gamma # γ\n",
    "        self.epsilon = epsilon # ϵ\n",
    "        self.s = np.zeros(dimension)\n",
    "\n",
    "    def step(self, gradient, x):\n",
    "        g = gradient(x)\n",
    "        self.s = self.gamma*self.s + (1 - self.gamma)*(g*g)\n",
    "        return x - self.alpha*g / (np.sqrt(self.s) + self.epsilon)"
   ]
  },
  {
   "source": [
    "## Algorithm 5.7"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adadelta:\n",
    "\n",
    "    def __init__(self, gamma_s, gamma_x, epsilon, dim):\n",
    "        self.gamma_s = gamma_s # γs\n",
    "        self.gamma_x = gamma_x # γx\n",
    "        self.epsilon = epsilon # ϵ\n",
    "        self.u = np.zeros(dim)\n",
    "        self.s = np.zeros(dim)\n",
    "\n",
    "    def step(self, gradient, x):\n",
    "        g = gradient(x)\n",
    "        self.s = self.gamma_s*self.s + (1 - self.gamma_s)*(g*g)\n",
    "        delta_x = ( ( np.sqrt(self.u) + self.epsilon )/( np.sqrt(self.s) + self.epsilon )) * g\n",
    "        self.u = self.gamma_x*self.u + (1 - self.gamma_x)*(delta_x*delta_x)\n",
    "\n",
    "        return x + delta_x        \n"
   ]
  },
  {
   "source": [
    "## Algorithm 5.8"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "\n",
    "    def __init__(self, alpha, gamma_v, gamma_s, epsilon, dim):\n",
    "        self.alpha = alpha # α\n",
    "        self.gamma_v = gamma_v # γv\n",
    "        self.gamma_s = gamma_s # γs\n",
    "        self.epsilon = epsilon # ϵ\n",
    "        self.k = 1\n",
    "        self.v = np.zeros(dim)\n",
    "        self.s = np.zeros(dim)\n",
    "\n",
    "\n",
    "    def step(self, gradient, x):\n",
    "        g = gradient(x)\n",
    "        self.v = self.gamma_v*self.v + (1 - self.gamma_v)*g\n",
    "        self.s = self.gamma_s*self.s + (1-self.gamma_s)*(g*g)\n",
    "        self.k += 1\n",
    "        v_hat = self.v / (1 - self.gamma_v**(self.k) )\n",
    "        s_hat = self.s / (1 - self.gamma_s**(self.k) )\n",
    "\n",
    "        return x - self.alpha*v_hat / (np.sqrt(s_hat) + self.epsilon)"
   ]
  },
  {
   "source": [
    "### Example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1.0008185 1.9991815 2.9991815]\n"
     ]
    }
   ],
   "source": [
    "def func(x):\n",
    "    return jax.numpy.sin(x[0]*x[1])+jax.numpy.exp(x[1]+x[2])-x[2]\n",
    "\n",
    "alpha = 0.0011\n",
    "gamma_v = 0.9\n",
    "gamma_s = 0.999\n",
    "epsilon = 1e-8\n",
    "dim = 3\n",
    "model = Adam(alpha, gamma_v, gamma_s, epsilon, dim)\n",
    "\n",
    "x_0 = np.array([1.0, 2.0, 3.0])\n",
    "gradient = jax.grad(func)\n",
    "x_1 = model.step(gradient, x_0)\n",
    "print(x_1)"
   ]
  },
  {
   "source": [
    "## Algorithm 5.9"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperGradientDescent:\n",
    "\n",
    "    def __init__(self, alpha, mu, dim):\n",
    "        self.alpha = alpha # α\n",
    "        self.mu = mu # μ\n",
    "        self.g_prev = np.zeros(dim) #previous gradient\n",
    "\n",
    "\n",
    "    def step(self, gradient, x):\n",
    "        g = gradient(x)\n",
    "        self.alpha += self.mu*(np.dot(g,self.g_prev))\n",
    "        self.g_prev = g\n",
    "\n",
    "        return x - self.alpha*g"
   ]
  },
  {
   "source": [
    "## Algorithm 5.10"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperNesterovMomentum:\n",
    "\n",
    "    def __init__(self, alpha, mu, beta, dim):\n",
    "        self.alpha = alpha # α\n",
    "        self.mu = mu # μ\n",
    "        self.beta = beta # β\n",
    "        self.v = np.zeros(dim)\n",
    "        self.g_prev = np.zeros(dim) #previous gradient\n",
    "\n",
    "    def step(self, gradient, x):\n",
    "        g = gradient(x)\n",
    "        self.alpha -= self.mu*(np.dot(g,-self.g_prev-self.beta*self.v))\n",
    "        self.v = self.beta*self.v + g\n",
    "        self.g_prev = g\n",
    "\n",
    "        return x - self.alpha*(g + self.beta*self.v)"
   ]
  }
 ]
}